{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64c3496-c231-4330-9568-edf894e6d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import bigquery as bq\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import storage\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93ddbc0d-98d9-4fd6-a50e-56a3bd621c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        '/Users/felipedemenechvasconcelos/keys/scenic-edition-310913-26647dbaf7a5.json')\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "911c034c-1714-492d-99b9-7a4a267b2b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: de_rent_bkt>]\n"
     ]
    }
   ],
   "source": [
    "explicit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4006c-00e8-4993-9bdf-5ec95da81a6b",
   "metadata": {},
   "source": [
    "## Teste cloud storage connection - get number of offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbe5e9d8-2cc2-4c75-9a71-d4f270aebd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting offers in Dusseldorf...\n",
      "Getting offers in Berlin...\n",
      "Getting offers in Essen...\n",
      "Getting offers in Munchen...\n",
      "Getting offers in Koln...\n",
      "Getting offers in Stuttgart...\n",
      "Getting offers in Dresden...\n",
      "Getting offers in Hannover...\n",
      "Getting offers in Dortmund...\n",
      "Getting offers in Frankfurt am Main...\n",
      "Getting offers in Hamburg...\n",
      "File ../data/offers_qtt_by_city.csv uploaded to de_rent_data.\n"
     ]
    }
   ],
   "source": [
    "def get_offers_qtt(save=True):\n",
    "    '''\n",
    "    Get offers quantity by city and store it in a postgres DB.\n",
    "    '''\n",
    "    \n",
    "    cities_dict = {\n",
    "        'Dusseldorf': 100207,\n",
    "        'Berlin': 87372,\n",
    "        'Essen': 102157,\n",
    "        'Munchen': 121673,\n",
    "        'Koln': 113144,\n",
    "        'Stuttgart': 143262,\n",
    "        'Dresden': 100051,\n",
    "        'Hannover': 109489,\n",
    "        'Dortmund': 99990,\n",
    "        'Frankfurt am Main': 105043,\n",
    "        'Hamburg': 109447\n",
    "    }\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    now2 = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    # get all offers quantity - haus und wohnung\n",
    "\n",
    "    cities_offers = []\n",
    "\n",
    "    for city, code in cities_dict.items():\n",
    "        \n",
    "        #logging.info(f'Getting offers in {city}...')\n",
    "        print(f'Getting offers in {city}...')\n",
    "\n",
    "        total_offers = 0\n",
    "\n",
    "        for i in range(1,3):\n",
    "            url = f'https://www.immonet.de/immobiliensuche/sel.do?&sortby=0&suchart=1&objecttype=1&marketingtype=2&parentcat={i}&city={code}'\n",
    "            #url = f'https://www.immonet.de/immobiliensuche/sel.do?parentcat={i}&objecttype=1&pageoffset=378&listsize=27&suchart=1&sortby=0&city={code}&marketingtype=2&page=1'\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "            # Find number of rent offers.\n",
    "            a = soup.select('ul', class_='tbl margin-auto margin-top-0 margin-bottom-0 padding-0')\n",
    "            \n",
    "            for i in a:\n",
    "                if 'Alle Orte' in i.text:\n",
    "                    c = i.text\n",
    "                    total_offers += int(re.findall('\\d+', c)[0])\n",
    "            #total_offers += int(re.search('\\d+', a).group())\n",
    "\n",
    "        cities_offers.append({'extraction_datetime': now, 'city': city, 'city_code': code, 'offers': total_offers})\n",
    "\n",
    "    # offers_by_page = len(soup.findAll('div', class_=\"col-xs-12 place-over-understitial sel-bg-gray-lighter\"))    \n",
    "    df_offers = pd.DataFrame(cities_offers)\n",
    "    #df_offers.to_csv(f'temp_data/total_offers_by_city_temp_file.csv', index=False)\n",
    "    if save:\n",
    "        #if not os.path.exists('../data'):\n",
    "            #os.makedirs('../data')\n",
    "        df_offers.to_csv('../data/offers_qtt_by_city.csv', index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def upload_blob():\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"de_rent_bkt\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = \"../data/offers_qtt_by_city.csv\"\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = \"de_rent_data/de_rent_data\"\n",
    "    \n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        '/Users/felipedemenechvasconcelos/keys/scenic-edition-310913-26647dbaf7a5.json')\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    \n",
    "def main():\n",
    "    get_offers_qtt()\n",
    "    upload_blob()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb7518-8d4c-4f1b-aafd-5f160ebb0ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Teste cloud storage connection - get all offer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9578189-ed2a-4297-b3ee-bee1ae320a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting offer ids for Dusseldorf...\n",
      "Getting offer ids for Berlin...\n",
      "Getting offer ids for Essen...\n",
      "Getting offer ids for Munchen...\n",
      "Getting offer ids for Koln...\n",
      "Getting offer ids for Stuttgart...\n",
      "Getting offer ids for Dresden...\n",
      "Getting offer ids for Hannover...\n",
      "Getting offer ids for Dortmund...\n",
      "Getting offer ids for Frankfurt am Main...\n",
      "Getting offer ids for Hamburg...\n",
      "File ../data/all_offers_ids.csv uploaded to de_rent_data/all_offers_ids.csv.\n"
     ]
    }
   ],
   "source": [
    "'''Get the ids from all rent offers in all determinated cities.\n",
    "\n",
    "This script gets all the ids from all rent offers in all the predeterminated cities\n",
    "and save it in a database table named \"all_offer_ids\" to further use.\n",
    "\n",
    "The previous table is dropped and a new table is created each time that it runs.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import requests\n",
    "import psycopg2\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import psycopg2.extras as extras\n",
    "from google.cloud import storage\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create log folder if not exists\n",
    "if not os.path.exists('Logs'):\n",
    "    os.makedirs('Logs')\n",
    "    \n",
    "logging.basicConfig(\n",
    "    filename='Logs/get_offer_ids.txt',\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%Y-%m_%d %H:%M:%S',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('get_offer_ids')\n",
    "\n",
    "\n",
    "def get_offer_ids(df, save=True):\n",
    "    '''Get all offer ids for each offer in each city\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: a dataframe with the number of offers in each city.\n",
    "        \n",
    "        save: default=False\n",
    "            save the returned dataframe locally or not.\n",
    "            \n",
    "    Return:\n",
    "    -------\n",
    "        Return a dataframe with all offer ids for each city with the type of the offer (wohnung/haus).\n",
    "    '''    \n",
    "    \n",
    "    ids_list = []\n",
    "    offers_by_page = 26\n",
    "\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    for c in range(len(df)):\n",
    "        city = df.loc[c]['city']\n",
    "        city_code = df.loc[c]['city_code']\n",
    "        offers = df.loc[c]['offers']\n",
    "        \n",
    "        print(f'Getting offer ids for {city}...')\n",
    "        logger.info(f'Getting offer ids for {city}...')\n",
    "\n",
    "        # get all offers ids for haus und wohnung in each city\n",
    "\n",
    "\n",
    "        # Get the number of pages to scrape - rounded to down\n",
    "        number_of_pages = math.floor(offers / offers_by_page)\n",
    "\n",
    "        # wohnung/haus code\n",
    "        l_opt = [1, 2]\n",
    "\n",
    "        for opt in l_opt:\n",
    "            for page in range(number_of_pages):\n",
    "                url = f\"https://www.immonet.de/immobiliensuche/sel.do?parentcat={opt}&objecttype=1&pageoffset=1&listsize=26&suchart=1&sortby=0&city={city_code}&marketingtype=2&page={page}\"\n",
    "                headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "                page = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "                offers_list_1page = soup.findAll('div', class_=\"col-xs-12 place-over-understitial sel-bg-gray-lighter\")\n",
    "                \n",
    "                for i in range(len(offers_list_1page)):\n",
    "                    try:\n",
    "                        if opt == 1:\n",
    "                            ids_list.append({'extraction_datetime': now, 'offer_id': offers_list_1page[i].find('a')['data-object-id'], 'city': city, 'city_code': city_code, 'type': 'wohnung'})\n",
    "                        if opt == 2:\n",
    "                            ids_list.append({'extraction_datetime': now, 'offer_id': offers_list_1page[i].find('a')['data-object-id'], 'city': city, 'city_code': city_code, 'type': 'haus'})\n",
    "                    except:\n",
    "                        logger.error(f'Error - id:{i}')\n",
    "                        pass\n",
    "                sleep(randint(1, 2))         \n",
    "        sleep(randint(1, 5))          \n",
    "\n",
    "    # Create a dataframe with the infos\n",
    "    df_ids = pd.DataFrame(ids_list)\n",
    "    df_ids.drop_duplicates(subset='offer_id', inplace=True)\n",
    "\n",
    "    # save as csv file\n",
    "    if save:\n",
    "        if not os.path.exists('../data'):\n",
    "            os.makedirs('../data')\n",
    "        output_dir = '../data/'\n",
    "        #now2 = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        filename = f'all_offers_ids.csv'\n",
    "        df_ids.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def upload_blob():\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"de_rent_bkt\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = \"../data/all_offers_ids.csv\"\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = \"de_rent_data/all_offers_ids.csv\"\n",
    "    \n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        '/Users/felipedemenechvasconcelos/keys/scenic-edition-310913-26647dbaf7a5.json')\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    \n",
    "def main():\n",
    "    # connection to database\n",
    "    df = pd.read_csv('../data/offers_qtt_by_city.csv')\n",
    "    \n",
    "    get_offer_ids(df)\n",
    "    upload_blob()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddfa01-867c-48d7-94a5-2249377da2f9",
   "metadata": {},
   "source": [
    "## Get all infos, preprocess it and load into cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b234dca1-bbec-42ee-b3e1-fde1692e6312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/all_offers_infos_pp.csv uploaded to de_rent_data/all_offers_infos_pp.csv.\n"
     ]
    }
   ],
   "source": [
    "'''Get the raw data from the DB, clean and organize it.\n",
    "\n",
    "This script gets the raw dataset with all rent offers in the predefinated cities\n",
    "in Germany, separate the meaningful information, clean it and organize it in \n",
    "different columns.\n",
    "\n",
    "Returns a new dataframe read to be used.\n",
    "'''\n",
    "    \n",
    "# imports\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# set log folder, files and object configs\n",
    "if not os.path.exists('Logs'):\n",
    "    os.makedirs('Logs')\n",
    "    \n",
    "logging.basicConfig(\n",
    "    filename='Logs/offers_infos_cleaner.txt',\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%Y-%m_%d %H:%M:%S',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('offers_infos_cleaner')\n",
    "\n",
    "\n",
    "def offers_infos_preprocess(df_raw, save=True):\n",
    "    '''Clean and separate meaningful infos\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "        df_raw: Dataframe to be cleaned\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "        Returns a new dataframe with the meaningful informations separated by columns\n",
    "        and cleaned.    \n",
    "    '''\n",
    "    # Separate into latitude (lat) and longitude (lng)\n",
    "    df_raw['lat'] = df_raw['lat_lng'].apply(lambda x: re.findall('\\d+.\\d+', x)[0])\n",
    "    df_raw['lng'] = df_raw['lat_lng'].apply(lambda x: re.findall('\\d+.\\d+', x)[1])\n",
    "\n",
    "    # drop original lat_lng column\n",
    "    df_raw.drop(columns='lat_lng', inplace=True)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for x in range(len(df_raw)):\n",
    "        infos_dict = {}  \n",
    "        \n",
    "        # get infos from df_raw\n",
    "        infos_dict['offer_id'] = df_raw['offer_id'][x]\n",
    "        infos_dict['extraction_date'] = df_raw['extraction_date'][x]\n",
    "        infos_dict['lat'] = df_raw['lat'][x]\n",
    "        infos_dict['lng'] = df_raw['lng'][x]\n",
    "\n",
    "        # preprocess the infos cell\n",
    "        b = df_raw['offer_infos'][x].replace('\\\\', '')\n",
    "        b = b.replace('{', '').replace('}', '')[4:]\n",
    "        b = b[:-1]\n",
    "        b = b.split(',')\n",
    "        \n",
    "        # get all meaningful infos and return it cleane and\n",
    "        # separated by columns.\n",
    "        for i in b:\n",
    "            # offer area\n",
    "            if 'area' in i:\n",
    "                try:\n",
    "                    i = i.replace('\"', '').replace(\"'\", \"\").replace('area:', '').replace(' ', '')\n",
    "                    infos_dict['area_m2'] = float(i)\n",
    "                except:\n",
    "                    infos_dict['area_m2'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about area.')\n",
    "            # if the offer is furnished or not\n",
    "            if 'mobex' in i:\n",
    "                if 'true' in i:\n",
    "                    infos_dict['furnished'] = 1\n",
    "                elif 'false' in i:\n",
    "                    infos_dict['furnished'] = 0\n",
    "                else:\n",
    "                    infos_dict['furnished'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about furniture.')\n",
    "            #else:\n",
    "            #    infos_dict['furnished'] = np.nan\n",
    "            #    logger.debug(f'Offer {i} has no information about furniture.')\n",
    "            # the offer zip code \n",
    "            if 'zip' in i:\n",
    "                try:\n",
    "                    infos_dict['zip_code'] = int(re.findall('\\d+', i)[0])\n",
    "                except:\n",
    "                    infos_dict['zip_code'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about zip_code.')\n",
    "            # offer category\n",
    "            if 'objectcat' in i:\n",
    "                try:\n",
    "                    infos_dict['main_category'] = re.findall('\\:\"\\w+\"', b[3])[0][1:].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['main_category'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about main category.')\n",
    "            # number of rooms\n",
    "            if 'rooms' in i:\n",
    "                try:\n",
    "                    infos_dict['rooms'] = float(re.findall('\\d+', i)[0])\n",
    "                except:\n",
    "                    infos_dict['rooms'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about number of rooms.')\n",
    "            # build yuear of construction\n",
    "            if 'buildyear' in i:\n",
    "                try:\n",
    "                    infos_dict['build_year'] = int(re.findall('\\d+', i)[0])\n",
    "                except:\n",
    "                    infos_dict['build_year'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about build construction year.')\n",
    "            # state\n",
    "            if 'fed' in i:\n",
    "                try:\n",
    "                    infos_dict['state'] = i.split(':')[1].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['state'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about state.')\n",
    "            # city\n",
    "            if 'city' in i:\n",
    "                try:\n",
    "                    infos_dict['city'] = i.split(':')[1].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['city'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about city.')\n",
    "            # offer sub-category\n",
    "            if 'obcat' in i:\n",
    "                try:\n",
    "                    infos_dict['sub_category'] = i.split(':')[1].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['sub_category'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about sub-category.')\n",
    "            # if the offer has or not a \"balcon\"- balcony\n",
    "            if 'balcn' in i:\n",
    "                if 'true' in i:\n",
    "                    infos_dict['balcony'] = 1\n",
    "                elif 'false' in i:\n",
    "                    infos_dict['balcony'] = 0\n",
    "                else:\n",
    "                    infos_dict['balcony'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about balcony.')\n",
    "            #else:\n",
    "            #    infos_dict['balcony'] = np.nan\n",
    "            #    logger.debug(f'Offer {i} has no information about balcony.')\n",
    "            # heat type\n",
    "            if 'heatr' in i:\n",
    "                try:\n",
    "                    infos_dict['heat_type'] = i.split(':')[1].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['heat_type'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about heat type.')\n",
    "            # offer title\n",
    "            if 'title' in i:\n",
    "                try:\n",
    "                    infos_dict['offer_title'] = i.split(':')[1].replace('\"', '')\n",
    "                except:\n",
    "                    infos_dict['offer_title'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about offer title.')\n",
    "            # if the offer has already a kitchen\n",
    "            if 'kitch' in i:\n",
    "                if 'true' in i:\n",
    "                    infos_dict['kitchen'] = 1\n",
    "                elif 'false' in i:\n",
    "                    infos_dict['kitchen'] = 0\n",
    "                else:\n",
    "                    infos_dict['kitchen'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about kitchen.')\n",
    "            #else:\n",
    "            #    infos_dict['kitchen'] = np.nan\n",
    "            #    logger.debug(f'Offer {i} has no information about kitchen.')\n",
    "            if 'gardn' in i:\n",
    "                if 'true' in i:\n",
    "                    infos_dict['garden'] = 1\n",
    "                elif 'false' in i:\n",
    "                    infos_dict['garden'] = 0\n",
    "                else:\n",
    "                    infos_dict['garden'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information about garden.')\n",
    "            #else:\n",
    "            #    infos_dict['garden'] = np.nan\n",
    "            #    logger.debug(f'Offer {i} has no information about garden.')\n",
    "            # offer rent price\n",
    "            if 'price' in i:\n",
    "                try:\n",
    "                    infos_dict['rent_price'] = float(re.findall('\\d+', i)[0])\n",
    "                except:\n",
    "                    infos_dict['rent_price'] = np.nan\n",
    "                    logger.debug(f'Offer {i} has no information rent price.')\n",
    "                    \n",
    "        # append the infos about the offer           \n",
    "        df_list.append(infos_dict)\n",
    "        logger.info(f'Offer no. {i} cleaned.')\n",
    "        \n",
    "    # create a new cleaned dataframe\n",
    "    df_pp = pd.DataFrame(df_list)\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('../data'):\n",
    "            os.makedirs('../data')\n",
    "        output_dir = '../data/'\n",
    "        filename = f'all_offers_infos_pp.csv'\n",
    "        df_pp.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "        \n",
    "\n",
    "    return None\n",
    "\n",
    "def upload_blob():\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"de_rent_bkt\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = \"../data/all_offers_infos_pp.csv\"\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = \"de_rent_data/all_offers_infos_pp.csv\"\n",
    "    \n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        '/Users/felipedemenechvasconcelos/keys/scenic-edition-310913-26647dbaf7a5.json')\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    df_raw = pd.read_csv('../data/all_offers_infos_raw.csv')\n",
    "    df_cleaned = offers_infos_preprocess(df_raw)\n",
    "    upload_blob()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec9150-9bc6-41c1-8202-81c30c18551a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
